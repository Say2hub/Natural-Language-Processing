{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###*60009220131 Sayantan Mukherjee D2-2*"
      ],
      "metadata": {
        "id": "L3VCpScOaJ5T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jePR8aLeZ91x",
        "outputId": "fef300a5-3645-4fda-e21b-e9629e95ae23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "nltk.download('gutenberg')\n",
        "shakespeare_corpus = [gutenberg.raw(file_id) for file_id in gutenberg.fileids() if 'shakespeare' in file_id.lower()]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('shakespeare')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('gutenberg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pux5NPVCbF-y",
        "outputId": "f7f669e5-e832-441a-a397-7763b409ef3d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_files = [file_id for file_id in gutenberg.fileids() if 'shakespeare' in file_id.lower()]\n",
        "shakespeare_texts = [gutenberg.raw(file_id) for file_id in shakespeare_files]"
      ],
      "metadata": {
        "id": "i9B5cNxfbPGr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize into words\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stopwords and punctuation\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [\n",
        "        word for word in words\n",
        "        if word not in stop_words and word not in string.punctuation\n",
        "    ]\n",
        "    return filtered_words"
      ],
      "metadata": {
        "id": "njyBzlWFeXGp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_corpus = [preprocess(text) for text in shakespeare_texts]"
      ],
      "metadata": {
        "id": "S3b7b3faecKZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts = Counter()\n",
        "for doc in preprocessed_corpus:\n",
        "    word_counts.update(doc)\n",
        "\n",
        "# Step 2: Extract Top 10 Words by Frequency\n",
        "top_10_words = [word for word, _ in word_counts.most_common(10)]\n",
        "print(\"Top 10 words By Occurence in Shakespeare context:\")\n",
        "print(top_10_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lh-xO6_Zee_R",
        "outputId": "6ddb3a24-6bf9-4f14-a05d-baff4508a05e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 words By Occurence in Shakespeare context:\n",
            "[\"'d\", 'haue', 'ham', 'thou', \"'s\", 'shall', 'lord', 'come', 'enter', 'king']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_tf(doc):\n",
        "    \"\"\"\n",
        "    Compute Term Frequency (TF) for a single document.\n",
        "    TF(t, d) = (Number of times term t appears in document d) / (Total number of terms in document d)\n",
        "    \"\"\"\n",
        "    total_words = len(doc)\n",
        "    tf = {word: doc.count(word) / total_words for word in set(doc)}\n",
        "    return tf"
      ],
      "metadata": {
        "id": "JnA198DFaUgB"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_idf(corpus):\n",
        "    \"\"\"\n",
        "    Compute Inverse Document Frequency (IDF) for the entire corpus.\n",
        "    IDF(t) = log_e(Total number of documents / (1 + Number of documents containing term t))\n",
        "    \"\"\"\n",
        "    num_docs = len(corpus)\n",
        "    idf = {}\n",
        "    for word in set(word for doc in corpus for word in doc):  # All unique words in the corpus\n",
        "        doc_count = sum(1 for doc in corpus if word in doc)\n",
        "        idf[word] = math.log(num_docs / (1 + doc_count))  # Add 1 to avoid division by zero\n",
        "    return idf"
      ],
      "metadata": {
        "id": "MGWHX4oiaYkZ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_tfidf(tf, idf):\n",
        "    \"\"\"\n",
        "    Compute TF-IDF for a single document using precomputed TF and IDF.\n",
        "    TF-IDF(t, d) = TF(t, d) * IDF(t)\n",
        "    \"\"\"\n",
        "    tfidf = {word: tf_val * idf.get(word, 0) for word, tf_val in tf.items()}\n",
        "    return tfidf"
      ],
      "metadata": {
        "id": "bG5CJi4HadsB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Compute IDF for the entire corpus\n",
        "    idf = compute_idf(preprocessed_corpus)\n",
        "    print(\"\\n--- Inverse Document Frequency (IDF) ---\")\n",
        "    for word, score in list(idf.items())[:10]:  # Show top 10 IDF scores\n",
        "        print(f\"{word}: {score:.4f}\")\n",
        "    print(\"\\nTerm Frequency (TF):\")\n",
        "    for word, score in list(tf.items())[:10]:  # Show top 10 TF scores\n",
        "      print(f\"{word}: {score:.4f}\")\n",
        "\n",
        "    # Compute TF-IDF for each document\n",
        "    tfidf_scores_per_doc = []\n",
        "    for doc in preprocessed_corpus:\n",
        "        tf = compute_tf(doc)\n",
        "        tfidf = compute_tfidf(tf, idf)\n",
        "        tfidf_scores_per_doc.append(tfidf)\n",
        "        aggregated_tfidf = Counter()\n",
        "    for tfidf in tfidf_scores_per_doc:\n",
        "        aggregated_tfidf.update(tfidf)\n",
        "\n",
        "    # Extract Top 10 Words by Aggregated TF-IDF Scores\n",
        "    top_10_words = [word for word, _ in aggregated_tfidf.most_common(10)]\n",
        "\n",
        "    # Display Results\n",
        "    print(\"\\n-----Top 10 Words by Aggregated TF-IDF Scores Across All Documents-----\")\n",
        "    for word in top_10_words:\n",
        "        print(f\"{word}: {aggregated_tfidf[word]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-lmBqh_aiOZ",
        "outputId": "af238d6c-5ab5-4f14-a538-bd1675f015f7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Inverse Document Frequency (IDF) ---\n",
            "serue: -0.2877\n",
            "image: 0.0000\n",
            "thirtie: 0.0000\n",
            "link: 0.4055\n",
            "wife: -0.2877\n",
            "haply: 0.4055\n",
            "constrained: 0.4055\n",
            "decius: 0.4055\n",
            "expectansie: 0.4055\n",
            "was't: -0.2877\n",
            "\n",
            "Term Frequency (TF):\n",
            "serue: 0.0002\n",
            "glory: 0.0001\n",
            "image: 0.0002\n",
            "non-pareill: 0.0001\n",
            "wife: 0.0033\n",
            "houses: 0.0001\n",
            "constrained: 0.0001\n",
            "fitnesse: 0.0001\n",
            "craues: 0.0001\n",
            "layes: 0.0001\n",
            "\n",
            "-----Top 10 Words by Aggregated TF-IDF Scores Across All Documents-----\n",
            "ham: 0.0085\n",
            "bru: 0.0055\n",
            "macb: 0.0053\n",
            "cassi: 0.0038\n",
            "cassius: 0.0030\n",
            "antony: 0.0027\n",
            "hamlet: 0.0025\n",
            "macbeth: 0.0024\n",
            "hor: 0.0024\n",
            "macd: 0.0023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Summary:\n",
        "\n",
        "In this experiment, we implemented the TF-IDF (Term Frequency-Inverse Document Frequency) model to analyze the importance of words in Shakespeare's works from the Gutenberg corpus.\n",
        "\n",
        "\n",
        "We implemented three core functions to compute TF, IDF, and TF-IDF:\n",
        "\n",
        "##Term Frequency (TF) :\n",
        "\n",
        "###Formula:\n",
        "\n",
        "TF(t,d)=\n",
        "(Number of times term t appears in document d) / (Total number of terms in document d )\n",
        "​\n",
        "\n",
        "Purpose: Measures how frequently a word appears in a specific document.\n",
        "\n",
        "\n",
        "##Inverse Document Frequency (IDF) :\n",
        "\n",
        "###Formula:\n",
        "IDF(t)=log( Total number of documents/\n",
        "1 + Number of documents containing term t)\n",
        "\n",
        "Purpose: Measures how rare or common a word is across the entire corpus.\n",
        "\n",
        "##TF-IDF :\n",
        "\n",
        "###Formula:\n",
        "TF-IDF(t,d)=TF(t,d)×IDF(t)\n",
        "\n",
        "Purpose: Combines TF and IDF to highlight words that are both frequent in a document and rare across the corpus.\n",
        "\n",
        "###*Key Observations*\n",
        "Words like \"hamlet\" had high TF-IDF scores because they were frequent in specific documents but rare across the corpus.\n",
        "\n",
        "Common words like \"king\" and \"queen\" had lower IDF scores due to their widespread presence in multiple documents."
      ],
      "metadata": {
        "id": "15kWF5ePnxZR"
      }
    }
  ]
}